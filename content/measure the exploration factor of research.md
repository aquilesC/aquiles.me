Assuming that there are [[problems with citation-based metrics]] in scientific work, the authors of [@bhattacharya2020] propose an alternative metric: exploration factor. 

They argue that it is possible to measure the novelty of research by doing a text-based analysis on the published papers. They can count the number of new terms that appear and on which ideas they build insights. 

They call this the "edge factor" and could be complimentary to the "impact factor". Perhaps a scientist focuses on novel ideas that are still to immature to generate "impact" but that are part of the [[Scientific Exploration and Play]]. 

The authors of [@bhattacharya2020] describe a possible method, also using [[machine learning]], to identify papers that have a high novelty even though this may not be reflected in the number of citations. 

!!! image medium
    ![Novelty and impact in pub med publications](/images//Screenshot 2021-02-23 at 20.38.06.png)
    
The idea (shown in the plot above) is that there is a correlation between novelty and number of citations, but by adding another dimension to measuring output, it is possible to justify funding research that not necessarily belong to the Region 1 (i.e. high-impact research.)