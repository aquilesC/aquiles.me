---
status: draft
published_on: None
---
ðŸ§  I am constantly bombarded by messages of companies throwing AI around to try to savage their products. But AI can't be better than the data devices generate, and careful examination of the thesis is important. 

I have come across many pitches of companies building measurement instruments trying to tackle various challenges. Readout devices and sensing techniques have inherent limitations, and blindly believing AI can be the solution is an extremely risky business proposition.

A common approach to identify cells (including bacteria) is through some form of spectroscopy. Fluorescence labelling and detection is ubiquitous, and new sensors are opening the door for applications of Raman scattering. 

However, both fluorescence and Raman have their drawbacks. From specificity to poor signal/noise, to high background levels. 

AI is a very powerful tool but it can't solve problems where the initial data is not of sufficient quality. 

I have seen many companies jumping at the wagon of generating vasts amounts of data to train algorithms. While little is discussed about whether that data has any insights on the properties that are being studied. 

This became especially notorious in startups, where there's a high likelihood that the AI expert and the data acquisition expert do not speak the same language. 

Every time I see a new project popping up claiming that "AI will solve X problem" I become a bit skeptical. Not always, though. 

Machine Learning and AI have been enabling progress in many fronts, but it's hard not to suffer from the shiny object syndrome. Only time will tell how mistaken I was. 
